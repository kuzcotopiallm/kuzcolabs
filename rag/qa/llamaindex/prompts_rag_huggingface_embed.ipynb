{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626cabf4-9fd5-4f59-8eb0-f8f752273263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/prompts/prompts_rag.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69fb01b3-30f2-4e42-81f8-5fa4f8d2f145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install llama-index-readers-file pymupdf\n",
    "# %pip install llama-index-embeddings-openai\n",
    "# %pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9240932b-8914-44b8-a26d-4717e519ed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c085d9-e3b0-4215-a34b-6b38a4063215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']='sk-111111111111111111111111111111111111111111111111'\n",
    "os.environ['OPENAI_API_BASE']='http://127.0.0.1:5000/v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a024a1-d0fb-41eb-a6d1-e6532a6653cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral-7B-Instruct-v0.2-8.0bpw-h8-exl2-2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "model_info_url = 'http://127.0.0.1:5000/v1/internal/model/info'\n",
    "resp = requests.get(model_info_url)\n",
    "model = resp.json()['model_name']\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6913903-f08b-4251-8ff3-ddd8b1616fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70968e13-bc11-47d6-a6f0-036aa7b659ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d8d1d5-2afa-465f-bf0e-8106fca30ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import PromptTemplate\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a19d444-927e-43e1-9c9a-e91b465894d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir data\n",
    "# !wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9acfbcf6-bd31-4ca2-8018-c2df54ea4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "628718bc-cb9f-4891-b494-a3d0f229ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6275fb0-ba69-45e2-ac8e-8c1d7c661216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e0107bb-4eeb-42be-b20c-146260fc2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "gpt4_llm = OpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db4f771-5727-40c8-97d3-6b31bd21049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df756a9f-d8d9-47d4-82ef-03bdff5d4fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee8be35e-5a7f-4567-8015-67f8dd339ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2856b6663b0414da3819568f29b1fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_emeds = embed_model.get_text_embedding(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2400b09a-d597-4ba0-b652-c6d8835d55fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_emeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc8b9981-8b92-4523-94da-004a0888d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01eb1116-69b9-4b18-9433-e5727ddc7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c239e5f-a054-4d1a-b01b-d46bbcde5c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69175317448a4fcfa549a56ba2c9c643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94db9db8ff8f4ebfa5afeef6bd592dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cff282bf8e5441fa533403b9ffc0e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb34c2e4df744d3b7a68df89408f0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda8406473f44441a81353ec344962fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9468dfc393e4923bf07de0981994a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80e6bb5b05e41b6a11c146724027377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9353c2b724fa41aea8f85e3a9f85a574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1506e3ca704bb4accd0c61af73ccbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47ccc139fff4ed8b6da0c915aeafe73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ea3f3cfdb44789a974e8064f1d39a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e6cfb40-6dbb-4ce0-ab0d-ae5c22ebf770",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"What are the potential risks associated with the use of Llama 2 as mentioned in the context?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d23e1b61-b7cb-461a-8be1-d29f3360e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=2, llm=gpt35_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "675be614-07d3-4b27-9d2e-1bf43f8d898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = index.as_retriever(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e46ed6f4-e901-4a1b-87d9-7f203658167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b631f12513f04260bc3d565fc9acda80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:5000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:5000/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "The potential risks associated with the use of Llama 2, as mentioned in the context, include the potential for biases inherited from the training data, such as underrepresentation of certain demographic groups, which could result in biased model generations. Additionally, the models carry risks related to safety violations, as mentioned in Section 4.4, which includes human evaluations of safety violations across adversarial prompts. It is important to note that these safety evaluations are subject to inherent biases due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbcf7e0d-bb69-4fba-b2ab-d2c1ed83e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3ae074d-cea1-4d5e-bc29-5c3b31a3223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict = query_engine.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caedf031-1408-42e0-8a1f-3dbb83d04a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feb508a-7a5d-4e76-ac5d-c35d2e418bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-llamaindex",
   "language": "python",
   "name": "venv-llamaindex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
