{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47926b04-b323-4859-b378-8b4167f29f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c36837d-e38a-4af4-b67e-d861bc856b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']='sk-111111111111111111111111111111111111111111111111'\n",
    "os.environ['OPENAI_API_BASE']='http://127.0.0.1:5000/v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800f5baf-5a40-4556-b374-21308e6f498e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral-7B-Instruct-v0.2-8.0bpw-h8-exl2-2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "model_info_url = 'http://127.0.0.1:5000/v1/internal/model/info'\n",
    "resp = requests.get(model_info_url)\n",
    "model = resp.json()['model_name']\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f101ec-003b-4bef-a529-e16a3e054799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import pickle\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d1fc061-66fb-4777-bb0f-17aadbe1609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e50e5d6d-5473-4c08-9063-fea05ec3a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e741953-350c-439d-ab96-cbdc38688691",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test-docs.pkl','rb') as fh:\n",
    "    docs = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07f761d-3193-4dd2-a7ac-26f6fc6ce372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42824"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09813692-d21d-4255-a641-528e710e777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04125646-701e-4402-af01-0212ce1c22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c4badf4-3e89-44e8-8d6e-d14e7a5f34f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121a4a60-ed1c-4dc8-8e15-f52dd47b53b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdbe16a0-4f57-4044-89a8-8ec8f9abbd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 7056}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "574a1457-8bf1-43c8-ace7-d1ca8e2512b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4c8c13e-b0da-4169-802e-c2b1292c90cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", model_kwargs = {'device': 'cuda:1'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "130084e0-0646-425e-9024-1569745c6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "651d2bf1-c4c6-4733-94f2-63cd759e85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\n",
    "    \"What are the approaches to Task Decomposition?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95fa068b-0650-42f7-bafd-a86afb1bee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24592b6e-5602-4969-bc48-ada06abdc406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96c590be-1458-489a-ae52-7b20c5387fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63cfd609-ffc3-4e6f-aca5-3b0ee6f126cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External call\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb8c302f-12d0-4530-8544-24cfaba9b34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9eeb149c-9494-48ec-b201-3f1e8786e873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt.invoke(\n",
    "        {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    "    ).to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f90f26b1-64c2-42a8-becc-eb42eb11e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a522ef51-2723-4118-a506-be0a828fe23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b6717ce-014e-4ee3-af55-9cb5a8be88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d3ea0ad-040b-4a9d-8394-365c480515a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex task into smaller, manageable sub-tasks. This can be done using language model prompts, task-specific instructions, or human inputs. For example, in the context of a Super Mario game in Python, tasks might include setting up the game environment, defining game functions, and implementing user input for keyboard control. Task decomposition allows agents to plan ahead and better understand their thought process. LLMs, such as CoT, utilize this technique to enhance model performance on complex tasks."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d015e-4053-46a8-a972-b3ffe0bf25c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "733d7f73-d36a-4f40-8458-a12af9f7d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e5cbbe4-8622-4f94-9dc9-d0af1f72db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be59e8ea-7404-4dc9-b698-7b6a06a8f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_custom = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15492374-37a2-492a-8756-f5072f82626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76965e0c-4307-4f3b-8a28-8b394819a582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task Decomposition is the process of breaking down complex tasks into smaller, manageable steps. It is a common technique used in AI systems to enhance performance and provide insight into the model's thought process. This can be achieved through LLM with simple prompts, task-specific instructions, or human inputs. (thanks for asking!)\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e5bbd43-be63-47f5-bd11-2b64be45f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebfe0d8e-5aed-49ff-a4e4-b6199d17db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ce4e240-9709-4add-a39d-bfc379d300f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4601427b-9a81-4001-ab5c-9d659c05be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8feb4042-d550-404d-b7d1-635d02090198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 2192},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 1585},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 17804},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 39221},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 30952},\n",
       "  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       "   'start_index': 4317}],\n",
       " 'answer': 'Task decomposition is the process of breaking down a complex task into smaller, manageable steps. It is a common technique used in artificial intelligence and machine learning models to enhance performance and provide insight into the model\\'s thinking process. CoT (Chain of Thought) is a popular prompting technique for task decomposition, instructing the model to \"think step by step\" to decompose big tasks into smaller tasks. (Thanks for asking!)'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_with_source.invoke(\"What is Task Decomposition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96b4512a-b21e-482f-b315-6cf255bfee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d603117-d785-47d7-82f1-6d896ce9addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25e5255b-cb19-49ad-99e1-af5b902f77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4d753c2-8162-4cec-b4b4-3a6a8f19ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_q_chain = condense_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf16108a-3975-41db-bfbc-56f3c536721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b841635-821f-4b30-99e0-34676b7044bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the context of language models, \"large\" refers to models that have been trained on a significant amount of data and have a large number of parameters. These models are able to generate more accurate and contextually relevant responses compared to smaller models. They are also able to handle a wider range of topics and have a deeper understanding of language.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condense_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "983e08b9-dcf7-4934-9c6f-dbbcbc189042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformer models are a type of neural network architecture introduced in a paper called \"Attention is All You Need\" by Vaswani et al. in 2017. The transformer model is designed for sequence-to-sequence tasks, such as machine translation, but it has also been applied to other tasks like text summarization and language modeling.\\n\\nThe key innovation of the transformer model is the self-attention mechanism, which allows the model to focus on different parts of the input sequence when producing each output token. This is in contrast to traditional recurrent neural networks (RNNs) and long short-term memory (LSTM) networks, which process the input sequence one token at a time and rely on explicit state representations to keep track of context.\\n\\nThe self-attention mechanism works by computing a weighted sum of the input embeddings for each output token, where the weights are determined by the similarity of the input embeddings to each other. This similarity is calculated using a dot product and scaled by the square root of the dimension of the input embeddings. The resulting scores are then passed through a softmax function to create a probability distribution over the input sequence.\\n\\nThe attention scores are then used to compute a weighted sum of the input embeddings for each output token, giving it more weight if the input embeddings are similar to each other and less weight if they are dissimilar.\\n\\nThe transformer model consists of multiple self-attention layers, each followed by a feed-forward neural network with two linear layers and a ReLU activation function. The output of the last self-attention layer is used as the final output of the model.\\n\\nDuring training, the transformer model is typically fine-tuned on a large dataset of parallel text, such as bilingual corpora for machine translation. The model is trained to minimize the cross-entropy loss between its predicted output and the ground truth translation.\\n\\nDuring inference, the transformer model is given an input sequence and generates an output sequence one token at a time, using the self-attention mechanism to attend to different parts of the input sequence when producing each output token.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condense_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"How do transformers work\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be215d9f-aed7-4e88-b798-1a50bf675f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7eaee248-89cd-4541-abf2-bae06e06d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc724e0f-e791-4118-bf7f-838548d12212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return condense_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4bc5107a-aff7-4cdd-a943-252312164741",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=condense_question | retriever | format_docs)\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96fbe14c-fc16-4308-aa02-e13bb5eb1523",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2a04eea-1d96-4b02-b674-12fecbb6ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45786c01-616e-46c8-a957-94be86cea750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There are several common ways to perform task decomposition, depending on the specific context and the nature of the project or problem being addressed. Here are some common approaches:\\n\\n1. Top-down decomposition: In this approach, the overall goal or objective is defined first, and then the project is broken down into progressively smaller and more detailed sub-tasks or components. This method is commonly used in software development projects, where the system architecture is defined first, and then the various components and modules are designed and developed.\\n2. Bottom-up decomposition: In this approach, the focus is on the individual components or tasks, and the project is built up from the ground level. This method is commonly used in data analysis or data processing projects, where large datasets are broken down into smaller pieces and processed in parallel.\\n3. Divide and Conquer: In this approach, the project is divided into smaller sub-problems, each of which is solved independently, and the solutions are then combined to form the overall solution. This method is commonly used in algorithms and problem-solving contexts, where the goal is to find an efficient solution to a complex problem.\\n4. Iterative decomposition: In this approach, the project is broken down into smaller tasks or sub-projects, which are then completed in iterations. Each iteration builds on the previous one, adding new features or functionality to the project. This method is commonly used in software development projects, where new features or functionality are added incrementally.\\n5. Parallel decomposition: In this approach, the project is broken down into smaller tasks or components that can be executed in parallel. This method is commonly used in high-performance computing, where large datasets or complex calculations are processed in parallel to improve performance.\\n\\nRegardless of the specific approach used, effective task decomposition requires a clear understanding of the overall project goals, the dependencies between tasks, and the available resources and constraints. Proper planning and communication are also essential to ensure that each task is completed effectively and efficiently, and that the overall project stays on track.', response_metadata={'token_usage': {'completion_tokens': 425, 'prompt_tokens': 341, 'total_tokens': 766}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_question = \"What are common ways of doing it?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97eb8144-d5c4-4a66-bf7d-346ae86e883a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is Task Decomposition?'),\n",
       " AIMessage(content='Task decomposition is a technique used in project management, computer science, and other fields to break down a complex project or task into smaller, manageable sub-tasks or components. This process helps simplify the overall goal into more manageable pieces, making it easier to plan, execute, and manage the work involved.\\n\\nIn software engineering, task decomposition is often used in designing algorithms or developing software. A complex problem is broken down into smaller, more manageable functions or procedures. Each function or procedure then performs a specific task, contributing to the overall solution.\\n\\nOne common method for task decomposition is the use of recursion, where a problem is solved by reducing it to a smaller instance of itself. For example, in a tree-traversal algorithm, a large data structure is broken down into smaller sub-trees, which are then processed recursively.\\n\\nAnother approach to task decomposition is the Divide and Conquer method, where a problem is split into smaller sub-problems, each of which is solved independently, and the solutions are then combined to form the overall solution. This method is commonly used in sorting algorithms, such as Merge Sort and Quick Sort.\\n\\nTask decomposition is an essential technique for managing large, complex projects, as it helps to identify dependencies, allocate resources, and prioritize tasks. By breaking down a project into smaller tasks, it becomes more manageable and easier to understand, which can lead to more effective planning, execution, and ultimately, successful project completion.', response_metadata={'token_usage': {'completion_tokens': 311, 'prompt_tokens': 15, 'total_tokens': 326}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7439a-1500-41f2-9725-45a606c38b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-langchain",
   "language": "python",
   "name": "venv-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
